{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashik927/Array-Of-Different-Types/blob/main/Copy_of_Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzM5BdDV8X-r"
      },
      "source": [
        "batch_size = 30  # batch size for the model\n",
        "\n",
        "filters = 128\n",
        "kernel_size = 3\n",
        "model_type = 'cnn_static'         # cnn_static and cnn_rand\n",
        "word2vec_dataset = 'bangla_word2vec.txt'     # bangla_wv_cbow_window5_min4.txt or bangla_wv_cbow_window3_min2.txt\n",
        "embedding_dims = 100\n",
        "is_embedding_trainable = False\n",
        "model = 'cnn'\n",
        "review_dataset = 'ecomerce.xlsx'\n",
        "number_of_category = 5\n",
        "number_of_epoch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-zesrXM4T7Y",
        "outputId": "8fc94835-fd25-4dac-a201-758009e92883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from gensim import models\n",
        "\n",
        "import keras.backend as K\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model, Input, Sequential\n",
        "from keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import Adam\n",
        "import Text_preprocessor\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEyYJMG28YBz"
      },
      "source": [
        "def accuracy_with_threshold(y_true, y_pred, threshold):\n",
        "   y_pred = K.cast(K.greater(y_pred, threshold), K.floatx())\n",
        "   return K.eval(K.mean(K.equal(y_true, y_pred)))\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    pr = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    f1_score = 2 * (pr * rec) / (pr + rec)\n",
        "    return f1_score\n",
        "\n",
        "def evaluate_model_individual(target_true,target_predicted):\n",
        "    report = classification_report(target_true,target_predicted)\n",
        "    print (report)\n",
        "    logging.info(report)\n",
        "    print (\"The accuracy score is {:.2%}\".format(accuracy_score(target_true,target_predicted)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGb2kFtu8YLJ"
      },
      "source": [
        "#The print(df.loc['name']) claims the label is not in index\n",
        "\n",
        "#And the v=df.loc[str(df['main_id']) == str(2456)]['code'].values says 'KeyError False'\n",
        "#df.loc[df['main_id']=='2456']\n",
        "\n",
        "def get_data_and_lebel():\n",
        "    reviews =pd.read_excel('ecomerce.xlsx')\n",
        "\n",
        "\n",
        "    x = reviews['Text'].values\n",
        "    y = reviews['Category'].values\n",
        "    my_set = list(sorted(set(y)))\n",
        "    Category = np.zeros(len(my_set))\n",
        "    if y[1] == 'product':\n",
        "        ind = my_set.index('product')\n",
        "        Category[ind] = 1\n",
        "    print(Category)\n",
        "\n",
        "    prev_rev = ''\n",
        "    prev_lebel = []\n",
        "    my_review = []\n",
        "    my_label = []\n",
        "    i = 0\n",
        "    for review in x:\n",
        "        if review == prev_rev:\n",
        "            Category = prev_lebel\n",
        "            for index, category in enumerate(my_set):\n",
        "                if y[i] == category:\n",
        "                    Category[index] = 1\n",
        "            my_label[-1] = Category\n",
        "            i += 1\n",
        "            prev_lebel = Category\n",
        "            continue\n",
        "\n",
        "        my_review.append(review)\n",
        "        Category = np.zeros(len(my_set))\n",
        "\n",
        "        for index, category in enumerate(my_set):\n",
        "            if y[i] == category:\n",
        "                Category[index] = 1\n",
        "\n",
        "        my_label.append(Category)\n",
        "        i += 1\n",
        "        prev_rev = review\n",
        "        prev_lebel = Category\n",
        "    return my_review, my_label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnCaaK8j8YOV",
        "outputId": "0d85af49-3f35-4a18-e755-da4a82a427c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x, y = get_data_and_lebel()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjG-RCl18YRX"
      },
      "source": [
        "x = [Text_preprocessor.clean_bangla_string(text) for text in x]\n",
        "max_document_length = max([len(text.split(\" \")) for text in x])\n",
        "x = np.array(x)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgWFtC1-8YUW"
      },
      "source": [
        "train_len = int(len(x) * 0.9)\n",
        "x_train = x[:train_len]\n",
        "y_train = y[:train_len]\n",
        "x_test = x[train_len:]\n",
        "y_test = y[train_len:]\n",
        "\n",
        "test_len = len(x_test) -5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6LAtsRt8YXc",
        "outputId": "e90092d9-9d71-45f5-e01d-7ddf9b5eabea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(list(x_train) + list(x_test))\n",
        "vocab_size = len(tok.word_index) +1\n",
        "word_index = tok.word_index\n",
        "x_train = tok.texts_to_sequences(x_train)\n",
        "x_test = tok.texts_to_sequences(x_test)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
        "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1544 train sequences\n",
            "172 test sequences\n",
            "Average train sequence length: 6\n",
            "Average test sequence length: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpBM-kihTeDA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYLbPMd78Yak"
      },
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=max_document_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_document_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7h_xE5t8Yd-"
      },
      "source": [
        "x_small = x_test[test_len:]\n",
        "y_small = y_test[test_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_qQ-hcI8Yhi",
        "outputId": "3ceaafa9-17f5-4c77-a514-55a71681dfab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (1544, 27)\n",
            "x_test shape: (172, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lUAV0yu8Yk7",
        "outputId": "76250490-e42c-471a-8b24-6ead9c214ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "if model_type == 'cnn_static':\n",
        "    # using word2vec\n",
        "    embeddings_index = {}\n",
        "    file = open(word2vec_dataset, 'r', encoding='utf8')\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    file.close()\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dims))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.uniform(-0.5, 0.5, embedding_dims)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 15010 word vectors.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-126711cce97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (50) into shape (100)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mMQM2ho8Yn8",
        "outputId": "5e9ea3fe-7bef-4581-cf66-0a2ff5980278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "my_model = Sequential()\n",
        "if model_type == 'cnn_static':\n",
        "    em = Embedding(len(word_index)+1, embedding_dims, weights=[embedding_matrix], input_length=max_document_length, trainable=is_embedding_trainable)\n",
        "else:\n",
        "    em = Embedding(vocab_size, embedding_dims, input_length=max_document_length)\n",
        "\n",
        "my_model.add(em)\n",
        "if model == 'cnn':\n",
        "    my_model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "    my_model.add(GlobalMaxPooling1D())\n",
        "else:\n",
        "    my_model.add(LSTM(filters, recurrent_dropout=0.2))\n",
        "\n",
        "my_model.add(Dropout(0.2))\n",
        "my_model.add(Dense(number_of_category, activation='sigmoid'))\n",
        "my_model.compile(loss='binary_crossentropy', optimizer=Adam(0.01), metrics=['accuracy', precision, recall,  f1_score])\n",
        "\n",
        "hist = my_model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=number_of_epoch, validation_data=(x_test, y_test))\n",
        "print(hist.history)\n",
        "# logging.info(hist.history)\n",
        "\n",
        "acc = my_model.evaluate(x_test, y_test)\n",
        "testing = my_model.metrics_names\n",
        "print('metrics name: ', testing)\n",
        "\n",
        "# print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "logging.info(testing)\n",
        "logging.info(acc)\n",
        "logging.info('....')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1544 samples, validate on 172 samples\n",
            "Epoch 1/10\n",
            "1544/1544 [==============================] - 1s 860us/step - loss: 0.5454 - acc: 0.7655 - precision: 0.6320 - recall: 0.0580 - f1_score: nan - val_loss: 0.5093 - val_acc: 0.7581 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: nan\n",
            "Epoch 2/10\n",
            "1544/1544 [==============================] - 0s 250us/step - loss: 0.5110 - acc: 0.7668 - precision: 0.5901 - recall: 0.0579 - f1_score: nan - val_loss: 0.5050 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 3/10\n",
            "1544/1544 [==============================] - 0s 246us/step - loss: 0.5081 - acc: 0.7676 - precision: 0.6195 - recall: 0.0581 - f1_score: nan - val_loss: 0.5046 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 4/10\n",
            "1544/1544 [==============================] - 0s 253us/step - loss: 0.5077 - acc: 0.7666 - precision: 0.6542 - recall: 0.0548 - f1_score: nan - val_loss: 0.5082 - val_acc: 0.7663 - val_precision: 0.7849 - val_recall: 0.0391 - val_f1_score: nan\n",
            "Epoch 5/10\n",
            "1544/1544 [==============================] - 0s 250us/step - loss: 0.5069 - acc: 0.7676 - precision: 0.6057 - recall: 0.0631 - f1_score: nan - val_loss: 0.5067 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 6/10\n",
            "1544/1544 [==============================] - 0s 248us/step - loss: 0.5060 - acc: 0.7694 - precision: 0.6357 - recall: 0.0657 - f1_score: nan - val_loss: 0.5055 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 7/10\n",
            "1544/1544 [==============================] - 0s 237us/step - loss: 0.5083 - acc: 0.7680 - precision: 0.6613 - recall: 0.0630 - f1_score: nan - val_loss: 0.5054 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 8/10\n",
            "1544/1544 [==============================] - 0s 245us/step - loss: 0.5053 - acc: 0.7687 - precision: 0.6878 - recall: 0.0667 - f1_score: nan - val_loss: 0.5036 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 9/10\n",
            "1544/1544 [==============================] - 0s 243us/step - loss: 0.5067 - acc: 0.7681 - precision: 0.6121 - recall: 0.0622 - f1_score: nan - val_loss: 0.5094 - val_acc: 0.7686 - val_precision: 0.6628 - val_recall: 0.0594 - val_f1_score: nan\n",
            "Epoch 10/10\n",
            "1544/1544 [==============================] - 0s 255us/step - loss: 0.5056 - acc: 0.7681 - precision: 0.6501 - recall: 0.0621 - f1_score: nan - val_loss: 0.5050 - val_acc: 0.7651 - val_precision: 0.6395 - val_recall: 0.0441 - val_f1_score: nan\n",
            "{'val_loss': [0.5092569045549216, 0.5050217737985212, 0.5046009452537049, 0.5082208046386408, 0.5067068376513415, 0.5054731975461162, 0.5054017114777898, 0.5036093606505283, 0.5093919156595718, 0.5050210200769957], 'val_acc': [0.7581395791020504, 0.7686046750046486, 0.7686046750046486, 0.766279116619465, 0.7686046750046486, 0.7686046750046486, 0.7686046750046486, 0.7686046750046486, 0.7686046750046486, 0.7651163062383962], 'val_precision': [0.0, 0.6627906789613325, 0.6627906789613325, 0.7848836793455967, 0.6627906789613325, 0.6627906789613325, 0.6627906789613325, 0.6627906789613325, 0.6627906789613325, 0.6395348663939986], 'val_recall': [0.0, 0.05937059778113698, 0.05937059778113698, 0.03914390170816765, 0.05937059778113698, 0.05937059778113698, 0.05937059778113698, 0.05937059778113698, 0.05937059778113698, 0.04412729011545347], 'val_f1_score': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'loss': [0.5454270757850588, 0.5110387794789255, 0.5081328401834236, 0.5076678623421205, 0.5069180801896851, 0.5060209416123251, 0.5082849573363294, 0.5053139467931165, 0.5067020081408283, 0.5055976035218165], 'acc': [0.7655440649220363, 0.766839399933815, 0.7676165990261217, 0.766580333749865, 0.7676166044306879, 0.7694300671778812, 0.7680052025484915, 0.7686528691664879, 0.7681347389604144, 0.7681347352544261], 'precision': [0.6320121830300346, 0.5901184292037253, 0.6194948099283357, 0.6542375960768504, 0.605731864319873, 0.6356865251002534, 0.6612694241839987, 0.6877775705501514, 0.61211602328046, 0.6501048583286414], 'recall': [0.05804435869949083, 0.05789634974316303, 0.05809167847809396, 0.054795376032378544, 0.06307858963849244, 0.06571896235251055, 0.06301076074946343, 0.06669272362240083, 0.06215599916846409, 0.06213191846009209], 'f1_score': [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]}\n",
            "172/172 [==============================] - 0s 128us/step\n",
            "metrics name:  ['loss', 'acc', 'precision', 'recall', 'f1_score']\n",
            "Test accuracy: [0.5050210502258566, 0.7651162799014601, 0.6976743964261787, 0.0441874411217002, nan]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCd9bCt8YrQ",
        "outputId": "862ebe01-ab5a-4eed-bb33-b0e322084e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "preds = my_model.predict(x_test)\n",
        "# preds[preds>=0.5] = 1\n",
        "# preds[preds<0.5] = 0\n",
        "print(preds)\n",
        "y_test = y_test.astype(np.float32)\n",
        "max_accuracy = 0\n",
        "optimum_threshold = 0.5\n",
        "for i in range(50, 100):\n",
        "    th = i/100\n",
        "    threshold_accuracy = accuracy_with_threshold(y_test, preds, th)\n",
        "    if max_accuracy < threshold_accuracy:\n",
        "        max_accuracy = threshold_accuracy\n",
        "        optimum_threshold = th\n",
        "\n",
        "print('Optimum threshold: %f and accuracy: %.3f' %(optimum_threshold, max_accuracy))\n",
        "\n",
        "y_test = y_test.astype(int)\n",
        "preds[preds>=0.5] = 1\n",
        "preds[preds<0.5] = 0\n",
        "evaluate_model_individual(y_test, preds)\n",
        "\n",
        "\n",
        "print('its done...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.13829541 0.23971379 0.5195904  0.15430778 0.22269258]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.11121184 0.35398516 0.48549402 0.09593496 0.19097182]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.13829541 0.23971379 0.5195904  0.15430778 0.22269258]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.11121184 0.35398516 0.48549402 0.09593496 0.19097182]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.11121184 0.35398516 0.48549402 0.09593496 0.19097182]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.13829541 0.23971379 0.5195904  0.15430778 0.22269258]\n",
            " [0.17145267 0.27286094 0.5539137  0.2166135  0.28620523]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.16114756 0.21578863 0.5771819  0.19170746 0.26416957]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521842 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.4096231  0.34035355 0.09521845 0.21015728]\n",
            " [0.1202417  0.40962312 0.34035355 0.09521843 0.21015729]]\n",
            "Optimum threshold: 0.520000 and accuracy: 0.766\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        25\n",
            "           1       0.00      0.00      0.00        67\n",
            "           2       0.75      0.14      0.23        65\n",
            "           3       0.00      0.00      0.00        13\n",
            "           4       0.00      0.00      0.00        38\n",
            "\n",
            "   micro avg       0.75      0.04      0.08       208\n",
            "   macro avg       0.15      0.03      0.05       208\n",
            "weighted avg       0.23      0.04      0.07       208\n",
            " samples avg       0.05      0.05      0.05       208\n",
            "\n",
            "The accuracy score is 4.07%\n",
            "its done...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lykJ0N3XH04t"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Lu6-X9H07X"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi33jsTyH0-K"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAPnLx1VH1Av"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVPUFfj1H1Dv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lAIXIvIH1GB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEk79IdBH1I_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AorBNnwQH1N6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRgW5c3H1Qr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MUy0xw9H1Tr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcCF6MO8H1Wp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR_fWG1XH1Mm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wtV4FV6H02b"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzoyoe5PZIMC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUsMZRVDZIPS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}